# Overview

This report summarizes the training setup, implementation details, and evaluation results for two modeling approaches applied to a biological sequence task:

1. A custom Transformer encoder trained on nucleotide sequences using a masked language modeling (MLM) objective and subsequently evaluated on a multi-label classification task.
2. A pretrained protein language model (PLM) baseline using the ESM-1b encoder combined with a downstream gradient-boosted classifier.

The goal is to compare performance, model efficiency, and practical trade-offs between a lightweight, task-specific transformer and a large pretrained baseline.

# Model configuration and training
## Custom Transformer Encoder
The custom model was kept lightweight with 2,630,656 parameters with 10MB disk size. Key training details include:
* Embedding dimension: 256
* Number of layers: 3
* Number of attention heads: 4
* Feedforward dimension: 512
* Dropout: 0.05

An earlier version of the model unintentionally omitted positional encodings (effectively a NoPE setup). This was corrected by adding Rotary Positional Embeddings (RoPE), applied to the query, key, and value (QKV) projections. Tokenization was performed using the DNABERT-2 tokenizer with a vocabulary size of 4096.

The model was trained on a dataset of nucleotide sequences using the MLM objective for 15 epochs with a effective batch size of 256 (with gradient accumulation). The AdamW optimizer was used with a learning rate of 2e-4. 

Weight tying was enabled between the input embedding matrix and the masked language modeling (MLM) output head, following Press & Wolf (2017). This reduces parameter count, enforces consistency between input and output token representations, and is commonly used in modern language models to improve sample efficiency and generalization.

Weight tying was enabled between the input embedding matrix and the masked language modeling (MLM) output head, following Press & Wolf (2017). This reduces parameter count, enforces consistency between input and output token representations, and is commonly used in modern language models to improve sample efficiency and generalization.

# Loss functions
The downstream multi-label classification task was trained using Asymmetric (Symmetric) Focal Loss (ASL) to explicitly address severe class imbalance. ASL reduces the contribution of easy negative examples while maintaining gradient signal for rare positive labels.

Negative focusing parameter: 4.0
Positive focusing parameter: 1.0
Class weights: Inverse frequency of each class in the training set and rescaled to range [0.1, 5.0]

## Pretraining Task (MLM)
* Objective: Masked Language Modeling (20% masking rate)
* Training data: ~200,000 nucleotide sequences
* Validation set: 1,200 sequences

### MLM Validation Metrics
* Training loss: 5.5724
* Validation loss: 5.4057
* Masked accuracy: 0.1625
* Top-5 accuracy: 0.2572

These results indicate reasonable learning dynamics for a small model trained from scratch, though performance is naturally limited by model capacity and dataset size.

# Downstream Classification Performance (Transformer Encoder)
After pretraining, the transformer encoder was evaluated on a multi-label classification task across different antibiotic resistance classes. The model was fine-tuned for 15 epochs with a batch size of 256 and a learning rate of 1e-4.

### Classification Results
* Training loss: 0.0018
* Validation loss: 0.0016
* Precision: 0.3233
* Recall: 0.7908
* F1 score: 0.4590
* Subset accuracy: 0.2392
* ROC-AUC: 0.6833
* Hamming loss: 0.0397
* Label Ranking Average Precision (LRAP): 0.6456
* Matthews Correlation Coefficient (MCC): 0.4904
* Mean predicted probability: 0.5704

##  Interpretation
The custom transformer encoder demonstrates moderate performance on the multi-label classification task, with a balanced trade-off between precision and recall. The ROC-AUC of 0.6833 indicates reasonable discriminative ability, though there is room for improvement, potentially through larger models or more extensive pretraining data.

# Baseline: Pretrained Protein Language Model (PLM-ARG)
## Model Details
The baseline model utilizes the ESM-1b encoder, a large pretrained protein language model with approximately 650 million parameters and a disk size of 2.5GB. The ESM-1b model was combined with a downstream gradient-boosted classifier (XGBoost) for the multi-label classification task.

```
XGBClassifier(
    learning_rate=0.1,
    max_depth=7,
    n_estimators=200,
    objective="binary:logistic",
)
```

### Classification Results (PLM-ARG)
* Precision: 0.0213
* Recall: 1.0000
* F1 score: 0.0417
* Subset accuracy: 0.0000
* ROC-AUC: 0.8640
* Hamming loss: 0.9787
* Label Ranking Average Precision (LRAP): 0.7252)

> Note: Explicit loss values were not logged; qualitative inspection suggests the optimization objective converged well.

## Interpretation
* The model achieves perfect recall, but at the cost of extremely low precision.
* Subset accuracy is zero, indicating poor exact-label prediction.
* High ROC-AUC and LRAP suggest good ranking performance, but threshold calibration is problematic.
* A threshold sweep was performed, confirming strong ranking but weak classification calibration.

# Comparative Analysis
| Metric               | Custom Transformer Encoder | Pretrained PLM-ARG (ESM-1b + XGBoost) |
|----------------------|----------------------------|------------------------------------------|
| Parameters           | 2,630,656                  | ~650 million                             |
| Disk Size           | 10 MB                      | 2.5 GB                                   |
| Precision            | 0.3233                     | 0.0213                                   |
| Recall               | 0.7908                     | 1.0000                                   |
| F1 Score             | 0.4590                     | 0.0417                                   |
| Subset Accuracy      | 0.2392                     | 0.0000                                  |
| ROC-AUC              | 0.6833                     | 0.8640                                  |
| Hamming Loss         | 0.0397                     | 0.9787                                  |
| LRAP                 | 0.6456                     | 0.7252                                  |
| MCC                  | 0.4904                     | N/A                                      |
| Mean Predicted Prob. | 0.5704                     | N/A                                      |


# Conclusion
* The custom transformer vastly outperforms the PLM baseline in terms of practical classification metrics (F1, subset accuracy) while being orders of magnitude smaller.
* In contrast, the large pretrained PLM-ARG baseline excels in recall and ranking metrics but suffers from extremely low precision and subset accuracy. This highlights the challenges of threshold calibration in multi-label settings with imbalanced data.
* The PLM baseline excels in ranking metrics, but its poor calibration severely limits usefulness without careful thresholding.
* RoPE positional encodings were critical for stabilizing and improving the transformerâ€™s performance.


# Threshold sweep analysis for PLM-ARG
Best threshold: 0.52 

* f1: 0.5365
* recall 0.7929
* precision: 0.405
* subset accuracy: 0.1813

This confirms that while the PLM-ARG model can rank labels effectively, its default thresholding leads to suboptimal classification performance. Careful threshold selection is essential for practical deployment.


# Next Steps
* Experiment with larger custom transformer architectures and more extensive pretraining datasets to close the performance gap with the PLM baseline.
* High performance of PLM ARG is expected cause its a functional classification task, and protein models are better suited for this than nucleotide models.
* However, the massive size of PLM ARG makes it impractical for many applications, highlighting the value of efficient, task-specific models.
* Additionally genome level classification with ORF extraction is another avenue to explore for improving performance on this task.
